![](img/LightGBM/LightGBM.PNG)



# LightGBM面试题

## 1. 简单介绍一下LightGBM？

LightGBM是一个梯度 boosting 框架，使用基于学习算法的决策树。 它可以说是分布式的，高效的。

从 LightGBM 名字我们可以看出其是轻量级（Light）的梯度提升机（GBM），其相对 XGBoost 具有训练速度快、内存占用低的特点。

LightGBM 是为解决GBDT训练速度慢，内存占用大的缺点，此外还提出了：

- 基于Histogram的决策树算法

- 单边梯度采样 Gradient-based One-Side Sampling(GOSS)

- 互斥特征捆绑 Exclusive Feature Bundling(EFB)

- 带深度限制的Leaf-wise的叶子生长策略

- 直接支持类别特征(Categorical Feature)

- 支持高效并行

- Cache命中率优化

## 2. 介绍一下直方图算法、直方图做差加速

 ### 直方图算法

 直方图算法就是使用直方图统计，将大规模的数据放在了直方图中，分别是每个bin中**样本的梯度之和** 还有就是每个bin中**样本数量**

 ![20220808152413](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220808152413.png)

![20220808152748](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220808152748.png)

![20220808155504](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220808155504.png)

- 首先确定对于每一个特征需要多少个箱子并为每一个箱子分配一个整数；

- 将浮点数的范围均分成若干区间，区间个数与箱子个数相等

- 将属于该箱子的样本数据更新为箱子的值

- 最后用直方图表示

优点：

1. **内存占用更小**：相比xgb不需要额外存储预排序，且只保存特征离散化后的值(整型)

2. **计算代价更小**: 相比xgb不需要遍历一个特征值就需要计算一次分裂的增益，只需要计算k次(k为箱子的个数)

3. **直方图做差加速**：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍

缺点：
1. **不能找到很精确的分割点**，但实际上决策树对于分割点的精确程度并不太敏感，而且较“粗”的分割点也`自带正则化的效果`（bin数量决定了正则化的程度，bin越少惩罚越严重，欠拟合风险越高），再加上boosting算法本身就是弱分类器的集成

### 直方图做差加速

**一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到**。利用这个方法，Lightgbm 可以在构造一个叶子（含有较少数据）的直方图后，可以用非常微小的代价得到它兄弟叶子（含有较多数据）的直方图。

![20220808161842](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220808161842.png)

## 3. 介绍生长策略Leaf-wise和 Level-wise

XGBoost 采用 **Level-wise**，策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但**实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂**

![20220808161934](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220808161934.png)

LightGBM采用**Leaf-wise**的增长策略，该策略每次从当前所有叶子中，**找到分裂增益最大的一个叶子，然后分裂，如此循环**。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，`得到更好的精度`；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合

![20220808161945](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220808161945.png)

Level-wise的做法会产生一些低信息增益的节点，浪费运算资源，但个对于防止过拟合挺有用。 而Leaf-wise能够追求更好的精度，降低误差，但是会带来过拟合问题。 那你可能问，那为啥还要用Leaf-wise呢？ 过拟合这个问题挺严重！ 但是能提高精度，作者也使用了max_depth来控制树的高度。 

其实敢用Leaf-wise还有一个原因就是Lightgbm在做数据合并，直方图和GOSS等各个操作的时候，其实都有`天然正则化`的作用，所以作者在这里使用Leaf-wise追求高精度是一个不错的选择。

## 4. 介绍单边梯度采样算法(GOSS)？

GOSS算法从**减少样本**的角度出发，**排除大部分小梯度的样本，仅用剩下的样本计算信息增益**（梯度小的样本，训练误差也比较小，说明数据已经被模型学习的很好了，梯度比较小的样本对于降低残差的作用效果不是太大，只需关注梯度高的样本），它是一种在减少数据量和保证精度上平衡的算法。

![20220808163317](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220808163317.png)

与此同时，为了**不改变数据的总体分布**，GOSS对要进行分裂的特征按照绝对值大小进行排序，选取最大的a个数据，在剩下梯度小的数据中选取b个，这b个数据乘以权重$\frac{1-a}{b}$，最后使用这a+b个数据计算信息增益（估计直方图）

![20220808163340](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220808163340.png)

![20220808163353](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220808163353.png)

在基于采样样本的估计直方图中可以发现Ni的总个数依然是8个， 虽然6个梯度小的样本中去掉了4个，留下了两个。 但是这2个样本在梯度上和个数上都进行了3倍的放大，所以可以防止采样对原数数据分布造成太大的影响， 这也就是论文里面说的将更多的注意力放在训练不足的样本上的原因。在不降低太多精度的同时，减少了样本数量，使得训练速度加快。


## 5. 介绍互斥特征捆绑算法(EFB)？

互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果**将一些特征进行融合绑定，则可以降低特征数量**。

`通常被捆绑的特征都是互斥的`（即特征不会同时为非零值，像one-hot），这样两个特征捆绑起来才不会丢失信息。如果两个特征并不是完全互斥（部分情况下两个特征都是非零值），可以用一个指标对特征不互斥程度进行衡量，称之为冲突比率，当这个值较小时，我们可以选择把不完全互斥的两个特征捆绑，而不影响最后的精度

![20220808163724](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220808163724.png)


## 6. 特征之间如何捆绑？

如何判断哪些特征应该绑在一起：

- 首先将所有的特征看成图的各个顶点，`将不相互独立的特征用一条边连起来`，边的权重就是两个相连接的特征的总冲突值（也叫矛盾数，就是这两个特征上同时不为0的样本个数）；
- 然后`按照节点的度`（度就是建图中边的条数，比如x3有3条边，则度为3）对特征降序排序，度越大，说明与其他特征的冲突越大；
- 对于每一个特征，遍历已有的特征簇，**如果发现该特征加入到特征簇中的矛盾数不超过某一个阈值，则将该特征加入到该簇中**。 如果该特征不能加入任何一个已有的特征簇，则新建一个簇，将该特征加入到新建的簇中。

![20220809094716](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220809094716.png)

特征绑定后，如何区分特征值来源于哪个特征:

- 比如，我们把特征A和B绑定到了同一个bundle里面(区间重叠了~)， A特征的原始取值区间[0,10), B特征原始取值区间[0,20), 这样如果直接绑定，那么会发现我从bundle里面取出一个值5， 就分不出这个5到底是来自特征A还是特征B了。 所以我们可以再B特征的`取值上加一个常量`10转换为[10, 30)，绑定好的特征取值就是[0,30), 我如果再从bundle里面取出5， 就一下子知道这个是来自特征A。这样就可以放心的融合特征A和特征B。

![20220809095821](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220809095821.png)

## 7. LightGBM是怎么支持类别特征？

* 离散特征建立直方图的过程 

  统计该特征下每一种离散值出现的次数，并从高到低排序，并过滤掉出现次数较少的特征值, 然后为每一个特征值，建立一个bin容器。

* 计算分裂阈值的过程 

  * 先看该特征下划分出的bin容器的个数，如果bin容器的数量小于4，直接使用one vs other方式, 逐个扫描每一个bin容器，找出最佳分裂点;

  * 对于bin容器较多的情况, 先进行过滤，只让子集合较大的bin容器参加划分阈值计算, 对每一个符合条件的bin容器进行公式计算
    $$
    \frac{该bin容器下所有样本的一阶梯度之和 }{ 该bin容器下所有样本的二阶梯度之和} + 正则项 
    $$
    
* **这里为什么不是label的均值呢？其实"label的均值"只是为了便于理解，只针对了学习一棵树且是回归问题的情况， 这时候一阶导数是Y, 二阶导数是1**)，得到一个值，根据该值对bin容器从小到大进行排序，然后分从左到右、从右到左进行搜索，得到最优分裂阈值。但是有一点，没有搜索所有的bin容器，而是设定了一个搜索bin容器数量的上限值，程序中设定是32，即参数max_num_cat。
  
* LightGBM中对离散特征实行的是many vs many 策略，这32个bin中最优划分的阈值的左边或者右边所有的bin容器就是一个many集合，而其他的bin容器就是另一个many集合。
  
* 对于连续特征，划分阈值只有一个，对于离散值可能会有多个划分阈值，每一个划分阈值对应着一个bin容器编号，当使用离散特征进行分裂时，只要数据样本对应的bin容器编号在这些阈值对应的bin集合之中，这条数据就加入分裂后的左子树，否则加入分裂后的右子树。

## 8. LightGBM的优缺点

优点：

- 直方图算法极大的降低了时间复杂度；
- 单边梯度算法过滤掉梯度小的样本，减少了计算量；
- 基于 Leaf-wise 算法的增长策略构建树，减少了计算量；
- 直方图算法将存储特征值转变为存储 bin 值，降低了内存消耗
- 互斥特征捆绑算法减少了特征数量，降低了内存消耗

缺点：

- LightGBM在Leaf-wise可能会长出比较深的决策树，产生过拟合
- LightGBM是基于偏差的算法，所以会对噪点较为敏感；



## 9. GBDT是如何做回归和分类的

- **回归**

  生成每一棵树的时候，第一棵树的一个叶子节点内所有样本的label的均值就是这个棵树的预测值，后面根据残差再预测，最后根据将第一棵树的预测值+权重*(其它树的预测结果)

  ![image-20210629173116854](../../../../../Library/Application Support/typora-user-images/image-20210629173116854.png)

* **分类**

  分类时针对样本有三类的情况，

  * 首先同时训练三颗树。
    - 第一棵树针对样本 x 的第一类，输入为（x, 0）。
    - 第二棵树输入针对样本 x 的第二类，假设 x 属于第二类，输入为（x, 1）。
    - 第三棵树针对样本 x 的第三类，输入为（x, 0）。
    - 参照 CART 的生成过程。输出三棵树对 x 类别的预测值 f1(x), f2(x), f3(x)。
  * 在后面的训练中，我们仿照多分类的逻辑回归，使用 softmax 来产生概率。
    - 针对类别 1 求出残差 f11(x) = 0 − f1(x)；
    - 类别 2 求出残差 f22(x) = 1 − f2(x)；
    - 类别 3 求出残差 f33(x) = 0 − f3(x)。
  * 然后第二轮训练，
    - 第一类输入为(x, f11(x))
    - 第二类输入为(x, f22(x))
    - 第三类输入为(x, f33(x))。
  * 继续训练出三棵树，一直迭代 M 轮，每轮构建 3 棵树。当训练完毕以后，新来一个样本 x1，我们需要预测该样本的类别的时候，便可使用 softmax 计算每个类别的概率。

  

## 参考资料

深入理解LightGBM https://mp.weixin.qq.com/s/zejkifZnYXAfgTRrkMaEww

决策树（下）——XGBoost、LightGBM（非常详细） - 阿泽的文章 - 知乎 https://zhuanlan.zhihu.com/p/87885678

Lightgbm如何处理类别特征： https://blog.csdn.net/anshuai_aw1/article/details/83275299

LightGBM 直方图优化算法：https://blog.csdn.net/jasonwang_/article/details/80833001
